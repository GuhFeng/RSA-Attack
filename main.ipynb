{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Event Extraction_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Import packeges`  \n",
    "Using spacy dependency parser model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy  # Use displacy to visualize the dependency tree\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')  # load language model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ignore warning, mainly ignore the warning about GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load data`  \n",
    "We load the corpus to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open(\"./data/dataset.txt\", mode='r') as file:\n",
    "    sentence = file.readline()\n",
    "    while sentence:\n",
    "        corpus += [sentence]\n",
    "        sentence = file.readline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Preprocess corpus`  \n",
    "We mainly use regular expression to process the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger(sentence: str):\n",
    "    trigger = []\n",
    "    tags = [' <e> ', ' </e> ']\n",
    "    for tag in tags:\n",
    "        lst = sentence.split(tag)\n",
    "        for i in range(0, len(lst) - 1):\n",
    "            trigger.append(lst[i].split(' ')[-1])\n",
    "    return trigger\n",
    "\n",
    "\n",
    "trigger = [get_trigger(sentence) for sentence in corpus]\n",
    "corpus = [re.sub(\" <[\\S]*> \", \" \", s) for s in corpus]\n",
    "corpus = [re.sub(\" \\. \", \".\", s) for s in corpus]\n",
    "corpus = [re.sub(\"[\\.]{3}\", \" ... \", s) for s in corpus]\n",
    "corpus = [re.sub(\" , \", \", \", s) for s in corpus]\n",
    "corpus = [re.sub(\" - \", \" \", s) for s in corpus]\n",
    "corpus = [re.sub(\"[ ]{2,}\", \" \", s) for s in corpus]\n",
    "corpus = [re.sub(\"\\n\", \"\", s) for s in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Get dependency tree`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The speed of dependency parsing is about 10 sentence per second.\n",
    "We can process about 1000 sentences one time and finish all test work \n",
    "to check the method we apply is really work. \n",
    "\"\"\"\n",
    "dependency = [nlp(sentence) for sentence in corpus[:10]]\n",
    "\n",
    "\n",
    "def transform(sentence):\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return (node.orth_, [(child.tag_, to_nltk_tree(child))\n",
    "                                 for child in node.children])\n",
    "        else:\n",
    "            return node.orth_\n",
    "\n",
    "    return [to_nltk_tree(sent.root) for sent in sentence.sents]\n",
    "\n",
    "\n",
    "trees = [transform(sentence) for sentence in dependency]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
